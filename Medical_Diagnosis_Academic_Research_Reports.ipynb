{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLe1kuOjhoPGzolathcHWs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fee48a3d"
      },
      "source": [
        "# Medical Diagnosis: Academic Research Reports\n",
        "Uploads multiple PDFs of academic research reports (sourced from Google Scholar + JSTOR), split their text into \"chunks\" of text strings, summarize each chunk using an LLM, and then enable semantic search over the embedded vectors of chunks based on a user input of symptoms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b409967"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "Install libraries for PDF processing, text splitting, and semantic search.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa9c5b8d",
        "outputId": "47ca4e01-8a15-4dc3-a01a-37f221063991"
      },
      "source": [
        "%pip install pypdf langchain sentence-transformers faiss-cpu transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.9.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2365fc6"
      },
      "source": [
        "## 1) Upload and load pdf documents\n",
        "\n",
        "Enables user upload multiple PDF files (I've uploaded 28 files that cover a variety of medical conditions) and then load their content\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e50ed29"
      },
      "source": [
        "# Import necessary libraries and define the function to load and combine text from multiple PDF files\n",
        "\n",
        "import gradio as gr\n",
        "from pypdf import PdfReader\n",
        "\n",
        "def load_pdfs(file_paths):\n",
        "    \"\"\"Loads text from multiple PDF files and combines it into a single string.\"\"\"\n",
        "    combined_text = \"\"\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            reader = PdfReader(f)\n",
        "            for page in reader.pages:\n",
        "                combined_text += page.extract_text()\n",
        "    return combined_text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "5f3e532a",
        "outputId": "3c4e0d09-6907-40c9-b866-c238fe061d7e"
      },
      "source": [
        "# Use Gradio interface to allow users to upload multiple PDF files and consolidate the text into a single line of text.\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=load_pdfs,\n",
        "    inputs=gr.File(label=\"Upload PDF Files\", file_count=\"multiple\", file_types=[\".pdf\"]),\n",
        "    outputs=\"text\",\n",
        "    title=\"PDF Text Extractor\",\n",
        "    description=\"Upload multiple PDF files to extract and combine their text content.\"\n",
        ")\n",
        "\n",
        "combined_text = iface.launch()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://734a968188e0779f9d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://734a968188e0779f9d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a9d778d"
      },
      "source": [
        "## 2) Split text into chunks\n",
        "\n",
        "Break down the text content of the PDF documents into smaller text strings, i.e. \"chunks.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49a69bd4",
        "outputId": "5c1b2d28-3ab8-4137-a236-ca8c83673fa1"
      },
      "source": [
        "import nltk\n",
        "# Download the required NLTK data (still good practice even if not strictly needed for RecursiveCharacterTextSplitter)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # Switch back to RecursiveCharacterTextSplitter\n",
        "import os\n",
        "\n",
        "# Define the path to the combined text file you will upload\n",
        "COMBINED_TEXT_FILE_PATH = \"combined_text.txt\" # Assumes the file is in the default Colab directory (/content/)\n",
        "\n",
        "combined_text = \"\"\n",
        "# Read the combined text from the file\n",
        "if os.path.exists(COMBINED_TEXT_FILE_PATH):\n",
        "    try:\n",
        "        # Try reading with utf-8 first\n",
        "        with open(COMBINED_TEXT_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "            combined_text = f.read()\n",
        "        print(f\"Successfully loaded text from '{COMBINED_TEXT_FILE_PATH}' with utf-8 encoding.\")\n",
        "    except UnicodeDecodeError:\n",
        "        # If utf-8 fails, try latin-1\n",
        "        try:\n",
        "            with open(COMBINED_TEXT_FILE_PATH, \"r\", encoding=\"latin-1\") as f:\n",
        "                combined_text = f.read()\n",
        "            print(f\"Successfully loaded text from '{COMBINED_TEXT_FILE_PATH}' with latin-1 encoding.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading from file '{COMBINED_TEXT_FILE_PATH}' with latin-1 encoding: {e}\")\n",
        "            combined_text = \"\" # Ensure combined_text is empty on error\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading from file '{COMBINED_TEXT_FILE_PATH}': {e}\")\n",
        "        combined_text = \"\" # Ensure combined_text is empty on error\n",
        "else:\n",
        "    print(f\"Error: File '{COMBINED_TEXT_FILE_PATH}' not found.\")\n",
        "    print(\"Please upload 'combined_text.txt' using the File Explorer on the left sidebar.\")\n",
        "\n",
        "# Proceed with splitting only if combined_text has content\n",
        "if combined_text:\n",
        "    # Define chunk size and overlap using variables\n",
        "    chunk_size = 10000  # Example chunk size\n",
        "    chunk_overlap = 400 # Example overlap\n",
        "\n",
        "    # Initialize RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "\n",
        "    # create_documents expects a list of strings or Document objects\n",
        "    # We pass a list containing our single combined_text string\n",
        "    document_chunks = text_splitter.create_documents([combined_text])\n",
        "\n",
        "    # Print chunk size and overlap from the variables\n",
        "    print(f\"Created {len(document_chunks)} document chunks with chunk_size={chunk_size} and chunk_overlap={chunk_overlap}.\")\n",
        "    # Display the first few chunks to verify\n",
        "    if len(document_chunks) > 0:\n",
        "        print(\"First chunk:\")\n",
        "        print(document_chunks[0].page_content[:500] + \"...\") # Print first 500 chars of the first chunk\n",
        "    if len(document_chunks) > 1:\n",
        "        print(\"\\nSecond chunk:\")\n",
        "        print(document_chunks[1].page_content[:500] + \"...\") # Print first 500 chars of the second chunk\n",
        "else:\n",
        "    print(\"No text content available to split into chunks.\")\n",
        "    document_chunks = [] # Ensure document_chunks is defined even if no text is loaded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded text from 'combined_text.txt' with latin-1 encoding.\n",
            "Created 212 document chunks with chunk_size=10000 and chunk_overlap=400.\n",
            "First chunk:\n",
            "Biochemistry and Biophysics Reports 36 (2023) 101578\n",
            "Available online 23 November 2023\n",
            "2405-5808/© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\n",
            "Acne vulgaris: A review of the pathophysiology, treatment, and recent \n",
            "nanotechnology based advances \n",
            "Mallikarjun Vasam\n",
            "a , *\n",
            ", Satyanarayana Korutla\n",
            "a\n",
            ", Raghvendra Ashok Bohara\n",
            "b , c , ** \n",
            "a\n",
            "Chaitanya (Deemed to Be University)-Pharmacy, Hanamkonda, Wa...\n",
            "\n",
            "Second chunk:\n",
            "tory lesions such as ulcers (pustules, nodules, cysts and papules). \n",
            "Non-inflammatory lesions are smaller and less pus-filled than in -\n",
            "flammatory lesions [ 22 , 23 ]. In addition, it was discovered that \n",
            "neutrophils produce reactive oxygen species (ROS), which damage \n",
            "the follicular epithelium and contribute to acne inflammation. This \n",
            "causes the follicular substances to be expelled into the dermis, \n",
            "resulting in a variety of inflammatory acne lesions [ 24 ]. \n",
            "e) DNA Methylation : Under environ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f903ff12"
      },
      "source": [
        "## Summarize chunks using llm\n",
        "\n",
        "### Subtask:\n",
        "Use an LLM to generate summaries for each text chunk.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ada74f2"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary classes for loading an LLM and for processing documents with an LLM, instantiate an LLM, load the summarization chain, and then iterate through the document chunks to generate summaries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrgTw0I6tM1d",
        "outputId": "c7889eb0-ec03-460e-a285-c8faeec52be7"
      },
      "source": [
        "%pip install langchain-community"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.9)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40cdbe33",
        "outputId": "d3347d61-24fa-4df7-afb0-0f7aed569bdd"
      },
      "source": [
        "from google.colab import userdata\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import os\n",
        "try:\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "except Exception as e:\n",
        "    print(f\"Error retrieving OpenAI API key from Colab secrets: {e}\")\n",
        "    openai_api_key = None # Ensure api_key is None if retrieval fails\n",
        "\n",
        "# Instantiate an LLM using the API key from colab secrets\n",
        "if openai_api_key:\n",
        "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)\n",
        "\n",
        "    summarize_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "\n",
        "    # Iterate through chunks and generate summaries of the chunks\n",
        "    summaries = []\n",
        "    if 'document_chunks' in globals() and document_chunks:\n",
        "        for i, chunk in enumerate(document_chunks):\n",
        "            print(f\"Summarizing chunk {i+1}/{len(document_chunks)}...\")\n",
        "            try:\n",
        "                summary = summarize_chain.run([chunk])\n",
        "                summaries.append({\"chunk\": chunk, \"summary\": summary})\n",
        "            except Exception as e:\n",
        "                print(f\"Error summarizing chunk {i+1}: {e}\")\n",
        "                summaries.append({\"chunk\": chunk, \"summary\": f\"Error summarizing chunk: {e}\"})\n",
        "\n",
        "        print(\"\\nGenerated Summaries:\")\n",
        "        for i, item in enumerate(summaries[:3]): # Print first 3 summaries\n",
        "            print(f\"--- Summary {i+1} ---\")\n",
        "            print(item['summary'])\n",
        "            print(\"-\" * 20)\n",
        "    else:\n",
        "        print(\"Error: 'document_chunks' not found or is empty. Please ensure the text splitting step was successful.\")\n",
        "\n",
        "else:\n",
        "    print(\"OpenAI API key not loaded. Please store your key in Colab secrets with the name 'OPENAI_API_KEY'.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing chunk 1/212...\n",
            "Summarizing chunk 2/212...\n",
            "Summarizing chunk 3/212...\n",
            "Summarizing chunk 4/212...\n",
            "Summarizing chunk 5/212...\n",
            "Summarizing chunk 6/212...\n",
            "Summarizing chunk 7/212...\n",
            "Summarizing chunk 8/212...\n",
            "Summarizing chunk 9/212...\n",
            "Summarizing chunk 10/212...\n",
            "Summarizing chunk 11/212...\n",
            "Summarizing chunk 12/212...\n",
            "Summarizing chunk 13/212...\n",
            "Summarizing chunk 14/212...\n",
            "Summarizing chunk 15/212...\n",
            "Summarizing chunk 16/212...\n",
            "Summarizing chunk 17/212...\n",
            "Summarizing chunk 18/212...\n",
            "Summarizing chunk 19/212...\n",
            "Summarizing chunk 20/212...\n",
            "Summarizing chunk 21/212...\n",
            "Summarizing chunk 22/212...\n",
            "Summarizing chunk 23/212...\n",
            "Summarizing chunk 24/212...\n",
            "Summarizing chunk 25/212...\n",
            "Summarizing chunk 26/212...\n",
            "Summarizing chunk 27/212...\n",
            "Summarizing chunk 28/212...\n",
            "Summarizing chunk 29/212...\n",
            "Summarizing chunk 30/212...\n",
            "Summarizing chunk 31/212...\n",
            "Summarizing chunk 32/212...\n",
            "Summarizing chunk 33/212...\n",
            "Summarizing chunk 34/212...\n",
            "Summarizing chunk 35/212...\n",
            "Summarizing chunk 36/212...\n",
            "Summarizing chunk 37/212...\n",
            "Summarizing chunk 38/212...\n",
            "Summarizing chunk 39/212...\n",
            "Summarizing chunk 40/212...\n",
            "Summarizing chunk 41/212...\n",
            "Summarizing chunk 42/212...\n",
            "Summarizing chunk 43/212...\n",
            "Summarizing chunk 44/212...\n",
            "Summarizing chunk 45/212...\n",
            "Summarizing chunk 46/212...\n",
            "Summarizing chunk 47/212...\n",
            "Summarizing chunk 48/212...\n",
            "Summarizing chunk 49/212...\n",
            "Summarizing chunk 50/212...\n",
            "Summarizing chunk 51/212...\n",
            "Summarizing chunk 52/212...\n",
            "Summarizing chunk 53/212...\n",
            "Summarizing chunk 54/212...\n",
            "Summarizing chunk 55/212...\n",
            "Summarizing chunk 56/212...\n",
            "Summarizing chunk 57/212...\n",
            "Summarizing chunk 58/212...\n",
            "Summarizing chunk 59/212...\n",
            "Summarizing chunk 60/212...\n",
            "Summarizing chunk 61/212...\n",
            "Summarizing chunk 62/212...\n",
            "Summarizing chunk 63/212...\n",
            "Summarizing chunk 64/212...\n",
            "Summarizing chunk 65/212...\n",
            "Summarizing chunk 66/212...\n",
            "Summarizing chunk 67/212...\n",
            "Summarizing chunk 68/212...\n",
            "Summarizing chunk 69/212...\n",
            "Summarizing chunk 70/212...\n",
            "Summarizing chunk 71/212...\n",
            "Summarizing chunk 72/212...\n",
            "Summarizing chunk 73/212...\n",
            "Summarizing chunk 74/212...\n",
            "Summarizing chunk 75/212...\n",
            "Summarizing chunk 76/212...\n",
            "Summarizing chunk 77/212...\n",
            "Summarizing chunk 78/212...\n",
            "Summarizing chunk 79/212...\n",
            "Summarizing chunk 80/212...\n",
            "Summarizing chunk 81/212...\n",
            "Summarizing chunk 82/212...\n",
            "Summarizing chunk 83/212...\n",
            "Summarizing chunk 84/212...\n",
            "Summarizing chunk 85/212...\n",
            "Summarizing chunk 86/212...\n",
            "Summarizing chunk 87/212...\n",
            "Summarizing chunk 88/212...\n",
            "Summarizing chunk 89/212...\n",
            "Summarizing chunk 90/212...\n",
            "Summarizing chunk 91/212...\n",
            "Summarizing chunk 92/212...\n",
            "Summarizing chunk 93/212...\n",
            "Summarizing chunk 94/212...\n",
            "Summarizing chunk 95/212...\n",
            "Summarizing chunk 96/212...\n",
            "Summarizing chunk 97/212...\n",
            "Summarizing chunk 98/212...\n",
            "Summarizing chunk 99/212...\n",
            "Summarizing chunk 100/212...\n",
            "Summarizing chunk 101/212...\n",
            "Summarizing chunk 102/212...\n",
            "Summarizing chunk 103/212...\n",
            "Summarizing chunk 104/212...\n",
            "Summarizing chunk 105/212...\n",
            "Summarizing chunk 106/212...\n",
            "Summarizing chunk 107/212...\n",
            "Summarizing chunk 108/212...\n",
            "Summarizing chunk 109/212...\n",
            "Summarizing chunk 110/212...\n",
            "Summarizing chunk 111/212...\n",
            "Summarizing chunk 112/212...\n",
            "Summarizing chunk 113/212...\n",
            "Summarizing chunk 114/212...\n",
            "Summarizing chunk 115/212...\n",
            "Summarizing chunk 116/212...\n",
            "Summarizing chunk 117/212...\n",
            "Summarizing chunk 118/212...\n",
            "Summarizing chunk 119/212...\n",
            "Summarizing chunk 120/212...\n",
            "Summarizing chunk 121/212...\n",
            "Summarizing chunk 122/212...\n",
            "Summarizing chunk 123/212...\n",
            "Summarizing chunk 124/212...\n",
            "Summarizing chunk 125/212...\n",
            "Summarizing chunk 126/212...\n",
            "Summarizing chunk 127/212...\n",
            "Summarizing chunk 128/212...\n",
            "Summarizing chunk 129/212...\n",
            "Summarizing chunk 130/212...\n",
            "Summarizing chunk 131/212...\n",
            "Summarizing chunk 132/212...\n",
            "Summarizing chunk 133/212...\n",
            "Summarizing chunk 134/212...\n",
            "Summarizing chunk 135/212...\n",
            "Summarizing chunk 136/212...\n",
            "Summarizing chunk 137/212...\n",
            "Summarizing chunk 138/212...\n",
            "Summarizing chunk 139/212...\n",
            "Summarizing chunk 140/212...\n",
            "Summarizing chunk 141/212...\n",
            "Summarizing chunk 142/212...\n",
            "Summarizing chunk 143/212...\n",
            "Summarizing chunk 144/212...\n",
            "Summarizing chunk 145/212...\n",
            "Summarizing chunk 146/212...\n",
            "Summarizing chunk 147/212...\n",
            "Summarizing chunk 148/212...\n",
            "Summarizing chunk 149/212...\n",
            "Summarizing chunk 150/212...\n",
            "Summarizing chunk 151/212...\n",
            "Summarizing chunk 152/212...\n",
            "Summarizing chunk 153/212...\n",
            "Summarizing chunk 154/212...\n",
            "Summarizing chunk 155/212...\n",
            "Summarizing chunk 156/212...\n",
            "Summarizing chunk 157/212...\n",
            "Summarizing chunk 158/212...\n",
            "Summarizing chunk 159/212...\n",
            "Summarizing chunk 160/212...\n",
            "Summarizing chunk 161/212...\n",
            "Summarizing chunk 162/212...\n",
            "Summarizing chunk 163/212...\n",
            "Summarizing chunk 164/212...\n",
            "Summarizing chunk 165/212...\n",
            "Summarizing chunk 166/212...\n",
            "Summarizing chunk 167/212...\n",
            "Summarizing chunk 168/212...\n",
            "Summarizing chunk 169/212...\n",
            "Summarizing chunk 170/212...\n",
            "Summarizing chunk 171/212...\n",
            "Summarizing chunk 172/212...\n",
            "Summarizing chunk 173/212...\n",
            "Summarizing chunk 174/212...\n",
            "Summarizing chunk 175/212...\n",
            "Summarizing chunk 176/212...\n",
            "Summarizing chunk 177/212...\n",
            "Summarizing chunk 178/212...\n",
            "Summarizing chunk 179/212...\n",
            "Summarizing chunk 180/212...\n",
            "Summarizing chunk 181/212...\n",
            "Summarizing chunk 182/212...\n",
            "Summarizing chunk 183/212...\n",
            "Summarizing chunk 184/212...\n",
            "Summarizing chunk 185/212...\n",
            "Summarizing chunk 186/212...\n",
            "Summarizing chunk 187/212...\n",
            "Summarizing chunk 188/212...\n",
            "Summarizing chunk 189/212...\n",
            "Summarizing chunk 190/212...\n",
            "Summarizing chunk 191/212...\n",
            "Summarizing chunk 192/212...\n",
            "Summarizing chunk 193/212...\n",
            "Summarizing chunk 194/212...\n",
            "Summarizing chunk 195/212...\n",
            "Summarizing chunk 196/212...\n",
            "Summarizing chunk 197/212...\n",
            "Summarizing chunk 198/212...\n",
            "Summarizing chunk 199/212...\n",
            "Summarizing chunk 200/212...\n",
            "Summarizing chunk 201/212...\n",
            "Summarizing chunk 202/212...\n",
            "Summarizing chunk 203/212...\n",
            "Summarizing chunk 204/212...\n",
            "Summarizing chunk 205/212...\n",
            "Summarizing chunk 206/212...\n",
            "Summarizing chunk 207/212...\n",
            "Summarizing chunk 208/212...\n",
            "Summarizing chunk 209/212...\n",
            "Summarizing chunk 210/212...\n",
            "Summarizing chunk 211/212...\n",
            "Summarizing chunk 212/212...\n",
            "\n",
            "Generated Summaries:\n",
            "--- Summary 1 ---\n",
            "The article explores the causes and treatment options for acne vulgaris, including traditional and nanotechnology-based approaches. It discusses factors contributing to acne, such as sebum production and inflammation, and addresses antibiotic resistance in treatment. The article also reviews recent advancements in acne therapy and the importance of innovative therapeutic techniques.\n",
            "--------------------\n",
            "--- Summary 2 ---\n",
            "Acne is a common skin condition characterized by inflammatory and non-inflammatory lesions. Neutrophils and DNA methylation play a role in acne inflammation. Acne vulgaris is the most prevalent form, with various types of lesions. Topical treatments like retinoids are commonly used to reduce sebum production and control comedone growth. Other treatment options include anti-inflammatory and antibacterial drugs.\n",
            "--------------------\n",
            "--- Summary 3 ---\n",
            "The text discusses various topical and systemic treatments for acne vulgaris, including retinoids, antibiotics, combinational therapies, and hormonal treatments. These treatments target different aspects of acne such as inflammation, bacterial growth, and sebum production. Combinational therapy is often more effective than monotherapy, but adverse effects may include skin irritation, redness, and bacterial resistance. Hormonal treatments are commonly used in adult females and adolescents with acne.\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3374769a"
      },
      "source": [
        "## Implement semantic search\n",
        "\n",
        "### Subtask:\n",
        "Set up a system to perform semantic search over the summarized chunks based on a user query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64d3cc4e"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary classes for embeddings and vector stores, initialize an embedding model, extract the summary texts, and create a FAISS index from the summaries and embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "525f008e",
        "outputId": "8535fec9-320a-4c7e-fd93-ba44837216d4"
      },
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# initialize embedding model\n",
        "embedding_model = HuggingFaceEmbeddings()\n",
        "\n",
        "# create a list of the summary texts from the summaries variable\n",
        "summary_texts = [item['summary'] for item in summaries if 'summary' in item]\n",
        "\n",
        "# create a FAISS index from the list of summaries and the initialized embedding model if valid summary\n",
        "if summary_texts:\n",
        "    vector_store = FAISS.from_texts(summary_texts, embedding_model)\n",
        "    print(\"FAISS index created successfully.\")\n",
        "else:\n",
        "    vector_store = None\n",
        "    print(\"No valid summaries found to create FAISS index.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3341637571.py:5: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embedding_model = HuggingFaceEmbeddings()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt_vRRZJtovL",
        "outputId": "6e783b2f-c30b-441c-8b1f-e3729b470cc6"
      },
      "source": [
        "# save the FAISS index to disk for later use.\n",
        "if vector_store is not None:\n",
        "    vector_store.save_local(\"faiss_index\")\n",
        "    print(\"FAISS index saved to disk as 'faiss_index'.\")\n",
        "else:\n",
        "    print(\"No FAISS index to save.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index saved to disk as 'faiss_index'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02a92e4a"
      },
      "source": [
        "## Query the data available from research reports\n",
        "\n",
        "Allow the user to input a query (i.e. symptoms that they have experienced + medications they are currently taking) and retrieve relevant summarized chunks.\n",
        "\n",
        "load the saved FAISS index, define user query, and perform a similarity search across embedded vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2717767",
        "outputId": "7edef143-ad7f-413a-a82d-9aa65cfd6a15"
      },
      "source": [
        "import nltk\n",
        "# Download the required NLTK data directly as suggested by the LookupError\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09c29e09",
        "outputId": "f0914d21-1f7f-4d04-ce1a-58d83a3fe1ce"
      },
      "source": [
        "%pip install faiss-cpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59382ac8"
      },
      "source": [
        "# Task\n",
        "Modify the Gradio interface to take the relevant chunks from the semantic search, pass them to an LLM with a prompt to act as a medical AI and suggest common medical conditions, and display the LLM's output as an AI-generated diagnosis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "8e7b41c9",
        "outputId": "0785330b-1f86-4065-dc46-32c12d55c2c2"
      },
      "source": [
        "import gradio as gr\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the path to the saved FAISS index\n",
        "FAISS_INDEX_PATH = \"faiss_index\"\n",
        "# Define the path to the CSV file with drug names\n",
        "DRUG_LIST_FILE_PATH = \"FAERS_Q125.csv\" # Assuming the CSV file is in the default Colab directory\n",
        "\n",
        "# Load the saved FAISS index\n",
        "try:\n",
        "    embeddings = HuggingFaceEmbeddings()\n",
        "    vector_store = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "    print(\"FAISS index loaded successfully.\")\n",
        "\n",
        "    # Load the OpenAI API key from Colab secrets\n",
        "    try:\n",
        "        openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving OpenAI API key from Colab secrets: {e}\")\n",
        "        openai_api_key = None # Ensure api_key is None if retrieval fails\n",
        "\n",
        "    # Load drug names from the CSV file\n",
        "    drug_list = []\n",
        "    if os.path.exists(DRUG_LIST_FILE_PATH):\n",
        "        try:\n",
        "            drug_df = pd.read_csv(DRUG_LIST_FILE_PATH)\n",
        "            # Assuming the drug names are in the first column of the CSV\n",
        "            # Convert all entries in the first column to strings, handling potential errors\n",
        "            drug_list = drug_df.iloc[:, 0].astype(str).tolist()\n",
        "            # Further filter out any remaining non-string or empty values just in case\n",
        "            drug_list = [str(drug).strip() for drug in drug_list if pd.notna(drug) and str(drug).strip()]\n",
        "\n",
        "            print(f\"Successfully loaded {len(drug_list)} drug names from '{DRUG_LIST_FILE_PATH}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading drug list from '{DRUG_LIST_FILE_PATH}': {e}\")\n",
        "            drug_list = []\n",
        "    else:\n",
        "        print(f\"Warning: Drug list file '{DRUG_LIST_FILE_PATH}' not found. Drug occurrences will not be checked.\")\n",
        "\n",
        "\n",
        "    # Define the medical AI prompt\n",
        "    medical_ai_prompt = \"\"\"You are a medical AI analyzing research text snippets.\n",
        "Based on the following text snippets, suggest common medical conditions or insights.\n",
        "This is an AI-generated suggestion and not a substitute for professional medical advice.\n",
        "\n",
        "Relevant Text Snippets:\n",
        "{text}\n",
        "\n",
        "Suggested Medical Conditions/Insights:\n",
        "\"\"\"\n",
        "    medical_ai_prompt_template = PromptTemplate(\n",
        "        template=medical_ai_prompt,\n",
        "        input_variables=[\"text\"]\n",
        "    )\n",
        "\n",
        "    def perform_search(user_query):\n",
        "        \"\"\"Performs semantic search based on the user query, checks for drug occurrences, generates AI diagnosis, and returns it.\"\"\"\n",
        "        # Ensure user_query is treated as a string from the start\n",
        "        user_query_str = str(user_query)\n",
        "\n",
        "        if not user_query_str.strip():\n",
        "            return \"Please enter your experienced symptoms or query.\"\n",
        "\n",
        "        if vector_store is None:\n",
        "             return \"FAISS index not loaded. Cannot perform search.\"\n",
        "\n",
        "        ai_diagnosis = \"\" # Initialize variable for AI diagnosis\n",
        "\n",
        "        try:\n",
        "            # Check for occurrences of drug names from the list in the user query\n",
        "            found_drugs = []\n",
        "            user_query_lower = user_query_str.lower()\n",
        "            for drug in drug_list:\n",
        "                # Ensure drug is a string and not empty before calling lower() and checking\n",
        "                if isinstance(drug, str) and drug.strip():\n",
        "                    try:\n",
        "                        if drug.lower() in user_query_lower:\n",
        "                            found_drugs.append(drug)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error checking drug '{drug}': {e}\") # Log any errors during check\n",
        "            if found_drugs:\n",
        "                print(f\"Found the following drugs in the query: {', '.join(found_drugs)}\")\n",
        "                # You can decide how to use this information, e.g., prioritize chunks related to these drugs\n",
        "\n",
        "            # Perform a similarity search based on the user query\n",
        "            relevant_chunks = vector_store.similarity_search(user_query_str, k=10) # Use the string version of user_query\n",
        "\n",
        "            # Optional: Further filter relevant_chunks based on found_drugs if desired\n",
        "            # Example:\n",
        "            # if found_drugs:\n",
        "            #    filtered_chunks = []\n",
        "            #    for chunk in relevant_chunks:\n",
        "            #        chunk_content = str(chunk.page_content).lower() # Ensure chunk content is string\n",
        "            #        if any(isinstance(drug, str) and drug.strip() and drug.lower() in chunk_content for drug in found_drugs):\n",
        "            #            filtered_chunks.append(chunk)\n",
        "            #    relevant_chunks = filtered_chunks\n",
        "            #    if not relevant_chunks:\n",
        "            #        return f\"No relevant information found related to the mentioned drugs: {', '.join(found_drugs)}\"\n",
        "\n",
        "\n",
        "            if relevant_chunks:\n",
        "                # Instantiate the LLM\n",
        "                if openai_api_key:\n",
        "                    llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)\n",
        "\n",
        "                    # Create a summarization chain with the prompt\n",
        "                    summarize_chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=medical_ai_prompt_template)\n",
        "\n",
        "                    # Generate AI diagnosis from relevant chunks\n",
        "                    try:\n",
        "                        ai_diagnosis = summarize_chain.run(relevant_chunks)\n",
        "                    except Exception as e:\n",
        "                        ai_diagnosis = f\"Error generating AI diagnosis: {e}\"\n",
        "                else:\n",
        "                     ai_diagnosis = \"OpenAI API key not loaded. Cannot generate AI diagnosis.\"\n",
        "            else:\n",
        "                ai_diagnosis = \"No relevant information found to generate a diagnosis.\"\n",
        "\n",
        "            # Return only the AI diagnosis\n",
        "            return ai_diagnosis\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"An error occurred during search: {e}\"\n",
        "\n",
        "\n",
        "    # Create a Gradio interface for the query\n",
        "    iface = gr.Interface(\n",
        "        fn=perform_search,\n",
        "        inputs=gr.Textbox(label=\"Enter your symptoms or query\"),\n",
        "        outputs=\"text\",\n",
        "        title=\"Medical Research Semantic Search\",\n",
        "        description=\"Enter symptoms or a medical query to find relevant information from the research reports. The system will check your query for known drug names.\"\n",
        "    )\n",
        "\n",
        "    # Launch the Gradio interface\n",
        "    print(\"\\nLaunching Gradio interface for query input...\")\n",
        "    iface.launch()\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the FAISS index: {e}\")\n",
        "    print(\"Could not load the FAISS index. Ensure 'faiss_index' directory exists and contains the index.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-257230517.py:18: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embeddings = HuggingFaceEmbeddings()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index loaded successfully.\n",
            "Warning: Drug list file 'drug_list.csv' not found. Drug occurrences will not be checked.\n",
            "\n",
            "Launching Gradio interface for query input...\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9e342fe182b2a8f8a4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9e342fe182b2a8f8a4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}